---
title: "Classify articles into covid vs. non-covid"
author:
  - name: Jihun NAM
  - email: jihun.nam.2020@mitb.sum.edu.sg

date: 04-21-2021
output:
  distill::distill_article:
    self_contained: False
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## [1] Prepare Dataset

### 1. Import Library.

```{r echo=TRUE, message=FALSE, warning=FALSE}
packages = c('tidyverse', 'readxl', 'dplyr', 'here', 'skimr', 'textdata',  
             'tm', 'topicmodels', 'tidytext', 'tibble','wordcloud', 'textstem', 'ggstatsplot',
             'ggpubr','corrplot','seriation', 'dendextend', 'heatmaply', 'parallelPlot',
             'tibbletime', 'anomalize', 'timetk')

for(p in packages){
  if(!require(p, character.only = T)){
  install.packages(p)
  }
  library(p, character.only = T)
}
```

### 2. Import data file

1) Import raw articles (94k), the target for classification

```{r}
article_raw <- read_excel("data/All_media.xlsx")

names(article_raw)[1] <- 'date'
names(article_raw)[10] <- 'site_name'

```


```{r}
# https://rstudio-conf-2020.github.io/r-for-excel/pivot-tables.html

# To look at summary statistics we’ve used summary, which is good for numeric columns, but it doesn’t give a lot of useful information for non-numeric data. So it means it wouldn’t tell us how many unique sites there are in this dataset. To have a look there I like using the skimr package:

skimr::skim(article_raw)

```

### 3. Handling datetime

```{r}
article_raw$date <- as.Date(article_raw$date)
article_raw$week <- as.numeric(format(as.Date(article_raw$date), "%V"))
article_raw$month <- as.numeric(format(as.Date(article_raw$date), "%m"))
```

### 4. Pivot table for whole year by site

```{r}
article_media <- article_raw %>% 
  group_by(site_name) %>% 
  summarise(no_of_article = n(), 
            mean_facebook_total =  mean(FacebookInteractions, na.rm= TRUE),
            sum_facebook_total =  sum(FacebookInteractions, na.rm= TRUE))
```

### 5. Pivot table for daily trend analysis

```{r}
article_daily <- article_raw %>% 
  group_by(date,  site_name) %>% 
  summarise(no_of_article = n(), 
            mean_facebook_total =  mean(FacebookInteractions, na.rm= TRUE), 
            sum_facebook_total =  sum(FacebookInteractions, na.rm= TRUE))
```

### 6. Pivot table for monthly trend analysis

```{r}
article_monthly <- article_raw %>% 
  group_by(month, site_name) %>% 
  summarise(no_of_article = n(), 
            mean_facebook_total =  mean(FacebookInteractions, na.rm= TRUE), 
            sum_facebook_total =  sum(FacebookInteractions, na.rm= TRUE))
```

### 7. Visualisation - monthly trend

```{r}
ggplot(data = article_monthly, aes(x = month, y = no_of_article, color = site_name)) +
  geom_line()

ggplot(data = article_monthly, aes(x = month, y = mean_facebook_total, color = site_name)) +
  geom_line()

```
### 8. Visualisation - daily trend

```{r}
ggplot(data = article_daily, aes(x = date, y = no_of_article, color = site_name)) +
  geom_line()
ggplot(data = article_daily, aes(x = date, y = mean_facebook_total, color = site_name)) +
  geom_line()


```
## Analysis

### 1. daily mean of facebook interaction by media

```{r}
set.seed(123)

article_media_daily <- article_raw %>% 
  group_by(site_name,date) %>% 
  summarise(no_of_article = n(), 
            mean_facebook_total =  mean(FacebookInteractions, na.rm= TRUE),
            sum_facebook_total =  sum(FacebookInteractions, na.rm= TRUE))

ggbetweenstats(
  data = article_media_daily,
  x = site_name,
  y = mean_facebook_total,
  type="np",
  mean.ci=TRUE,
  pairwise.comparisons = TRUE,
  pairwise.display = "s",
  p.adjust.method = "fdr",
  message=FALSE,
  title = "mean_facebook_total"
)

```

### 2. facebook interaction by media (individual articles)

```{r}
set.seed(123)

ggbetweenstats(
  data = article_raw,
  x = site_name,
  y = FacebookInteractions,
  type="np",
  mean.ci=TRUE,
  pairwise.comparisons = TRUE,
  pairwise.display = "s",
  p.adjust.method = "fdr",
  message=FALSE,
  title = "mean_facebook_total"
)

```
### Visualisation of trend

```{r}
p <- ggplot(data = subset(article_raw, site_name == 'Channel News Asia'),
            mapping = aes(x = date, y = FacebookInteractions, label = Headline))

p + geom_point() + geom_smooth()
## `geom_smooth()` using method = 'loess' and formula 'y ~ x'
```

#### pivot one Media & date
```{r}
df_tmp <- article_raw %>% 
  filter(site_name == 'Channel News Asia') %>%             
  group_by(date) %>% 
  summarise(no_of_article = n(), 
            mean_facebook_total =  mean(FacebookInteractions, na.rm= TRUE))

df_tmp2 <- df_tmp %>% 
  select(date, mean_facebook_total)

df_tibble <- as_tibble(df_tmp2)
class(df_tibble)

```


## Using the ‘anomalize’ package

The R ‘anomalize’ package enables a workflow for detecting anomalies in data. The main functions are time_decompose(), anomalize(), and time_recompose().

```{r}
df_anomalized <- df_tibble %>%
    time_decompose(mean_facebook_total, merge = TRUE) %>%
    anomalize(remainder) %>%
    time_recompose()
df_anomalized %>% glimpse()

```

### Visualize the Anomalies

We can then visualize the anomalies using the plot_anomalies() function. 

```{r}
df_anomalized %>% 
  plot_anomalies(ncol = 3, alpha_dots = 0.75)

```
### Adjusting Trend and Seasonality

With anomalize, it’s simple to make adjustments because everything is done with a date or DateTime information so you can intuitively select increments by time spans that make sense (e.g. “5 minutes” or “1 month”).
First, notice that a frequency and a trend were automatically selected for us. This is by design. The arguments frequency = “auto” and trend = “auto” are the defaults. We can visualize this decomposition using plot_anomaly_decomposition().

```{r}
p1 <- df_anomalized %>%
    plot_anomaly_decomposition() +
    ggtitle("Freq/Trend = 'auto'")
p1

```
When “auto” is used, a get_time_scale_template() is used to determine the logical frequency and trend spans based on the scale of the data. You can uncover the logic:

```{r}
get_time_scale_template()

```
This implies that if the scale is 1 day (meaning the difference between each data point is 1 day), then the frequency will be 7 days (or 1 week) and the trend will be around 90 days (or 3 months). This logic can be easily adjusted in two ways: Local parameter adjustment & Global parameter adjustment.

### Adjusting Local Parameters

Local parameter adjustment is performed by tweaking the in-function parameters. Below we adjust trend = “2 weeks” which makes for a quite overfit trend.

```{r}
p2 <- df_tibble %>%
    time_decompose(mean_facebook_total,
                   frequency = "auto",
                   trend     = "2 weeks") %>%
    anomalize(remainder) %>%
    plot_anomaly_decomposition() +
    ggtitle("Trend = 2 Weeks (Local)")
# Show plots
p1
p2

```

### Adjusting the Global Parameter

We can also adjust globally by using set_time_scale_template() to update the default template to one that we prefer. We’ll change the “3 month” trend to “2 weeks” for time scale = “day”. Use time_scale_template() to retrieve the time scale template that anomalize begins with, mutate() the trend field in the desired location, and use set_time_scale_template() to update the template in the global options. We can retrieve the updated template using get_time_scale_template() to verify the change has been executed properly.

```{r}
time_scale_template() %>%
    mutate(trend = ifelse(time_scale == "day", "2 weeks", trend)) %>%
    set_time_scale_template()
get_time_scale_template()

```

Finally, we can re-run the time_decompose() with defaults, and we can see that the trend is “2 weeks”.
```{r}
p3 <- df_tibble %>%
    time_decompose(mean_facebook_total) %>%
    anomalize(remainder) %>%
    plot_anomaly_decomposition() +
    ggtitle("Trend = 2 Weeks (Global)")
p3

```
### Extracting the Anomalous Data Points

Now, we can extract the actual datapoints which are anomalies. For that, the following code can be run.

```{r}
df_tibble %>% 
  time_decompose(mean_facebook_total) %>%
  anomalize(remainder) %>%
  time_recompose() %>%
  filter(anomaly == 'Yes')
```

```{r}
article_raw %>% 
  filter(site_name == 'Channel News Asia' & date == '2020-03-29') %>% 
  select(Headline, FacebookInteractions) %>% 
  arrange(desc(FacebookInteractions))

```

### Adjusting Alpha and Max Anoms

The alpha and max_anoms are the two parameters that control the anomalize() function. H

#### Alpha

We can adjust alpha, which is set to 0.05 by default. By default, the bands just cover the outside of the range.

```{r}
p4 <- df_tibble %>%
    time_decompose(mean_facebook_total) %>%
    anomalize(remainder, alpha = 0.05, max_anoms = 0.2) %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE) +
    ggtitle("alpha = 0.05")
#> frequency = 7 days
#> trend = 91 days
p4

```
If we decrease alpha, it increases the bands making it more difficult to be an outlier. Here, you can see that the bands have become twice big in size.

```{r}
p5 <- df_tibble %>%
    time_decompose(mean_facebook_total) %>%
    anomalize(remainder, alpha = 0.025, max_anoms = 0.2) %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE) +
    ggtitle("alpha = 0.05")
#> frequency = 7 days
#> trend = 91 days
p5

```
### Max Anoms

The max_anoms parameter is used to control the maximum percentage of data that can be an anomaly. Let’s adjust alpha = 0.3 so pretty much anything is an outlier. Now let’s try a comparison between max_anoms = 0.2 (20% anomalies allowed) and max_anoms = 0.05 (5% anomalies allowed).

```{r}
p6 <- df_tibble %>%
    time_decompose(mean_facebook_total) %>%
    anomalize(remainder, alpha = 0.3, max_anoms = 0.2) %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE) +
    ggtitle("20% Anomalies")
#> frequency = 7 days
#> trend = 91 days
p7 <- df_tibble %>%
    time_decompose(mean_facebook_total) %>%
    anomalize(remainder, alpha = 0.3, max_anoms = 0.05) %>%
    time_recompose() %>%
    plot_anomalies(time_recomposed = TRUE) +
    ggtitle("5% Anomalies")
#> frequency = 7 days
#> trend = 91 days
p6
p7

```
## Using the ‘timetk’ package

It is a ToolKit for working with Time Series in R, to plot, wrangle, and feature engineer time series data for forecasting and machine learning prediction.

### Interactive Anomaly Visualization

Here, timetk’s plot_anomaly_diagnostics() function makes it possible to tweak some of the parameters on the fly.

```{r}
df_tibble %>% timetk::plot_anomaly_diagnostics(date, mean_facebook_total, .facet_ncol = 2)

```

### Interactive Anomaly Detection

To find the exact data points that are anomalies, we use tk_anomaly_diagnostics() function.

```{r}
df_tibble %>% timetk::tk_anomaly_diagnostics(date, mean_facebook_total) %>% filter(anomaly=='Yes')

```

## Conclusion

In this article, we have seen some of the popular packages in R that can be used to identify and visualize anomalies in a time series. To offer some clarity of the anomaly detection techniques in R, we did a case study on a publicly available dataset. There are other methods to detect outliers and those can be explored too.

```{r}


```

```{r}


```

```{r}


```


